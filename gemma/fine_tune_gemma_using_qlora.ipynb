{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta24789/llms-fine-tuning/blob/main/gemma/fine_tune_gemma_using_qlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zkx6eGs_eq9"
      },
      "source": [
        "## Objective\n",
        "\n",
        "- Fine tune **google/gemma-2b** model.\n",
        "- Dataset : **b-mc2/sql-create-context**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywhbvEHj_erB"
      },
      "outputs": [],
      "source": [
        "# !pip3 install -q -U bitsandbytes==0.42.0\n",
        "# !pip3 install -q -U peft==0.8.2\n",
        "# !pip3 install -q -U trl==0.7.10\n",
        "# !pip3 install -q -U accelerate==0.27.1\n",
        "# !pip3 install -q -U datasets==2.17.0\n",
        "# !pip3 install -q -U transformers==4.38.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXfRChCL_erD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkk9lsIi_erD",
        "outputId": "25df3697-32be-479e-e64f-6c248a33021b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from pprint import pprint\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments,pipeline,logging\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftModel\n",
        "from trl import SFTTrainer\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXL7tcAJ_erE"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGauIk4p_erF",
        "outputId": "a2827f4c-0846-490f-dde2-0aa17e31af64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['context', 'answer', 'question'],\n",
              "    num_rows: 78577\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpF2e9jw_erG",
        "outputId": "4ce79cfe-3e0b-4748-9c41-68ce25e80fe1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 70719\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 7858\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Create Train & Test Split\n",
        "dataset = dataset.train_test_split(test_size = 0.1, seed=42)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v29A_scz_erG",
        "outputId": "6d486629-7185-4f05-fe44-dc9366c47c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE table_name_75 (insurgents VARCHAR, civilians VARCHAR)',\n",
              " 'answer': 'SELECT insurgents FROM table_name_75 WHERE civilians = \"49\"',\n",
              " 'question': 'Name the insurgents for civilians being 49'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afgi5B-Q_erH"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "Data Format:\n",
        "\n",
        "        Question : <question>\n",
        "        Context : <context>\n",
        "        Answer : <answer>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8wuxTnr_erH"
      },
      "outputs": [],
      "source": [
        "def transform_data(row):\n",
        "    text = f\"Question : {row['question']}\\nContext : {row['context']}\\nAnswer : {row['answer']}\"\n",
        "    return {\"text\":text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL_YZsMy_erI",
        "outputId": "e188d396-9c53-4ed5-a4d8-1adc648ab0cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question', 'text'],\n",
              "        num_rows: 70719\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['context', 'answer', 'question', 'text'],\n",
              "        num_rows: 7858\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformed_dataset = dataset.map(transform_data)\n",
        "transformed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIxAURvy_erJ",
        "outputId": "1f07718c-072f-44bd-9809-a79bd7e147d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question : Name the insurgents for civilians being 49\n",
            "Context : CREATE TABLE table_name_75 (insurgents VARCHAR, civilians VARCHAR)\n",
            "Answer : SELECT insurgents FROM table_name_75 WHERE civilians = \"49\"\n"
          ]
        }
      ],
      "source": [
        "print(transformed_dataset['train'][0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrrzdtxa_erJ"
      },
      "source": [
        "## Load Model & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo1W-t3M_erK",
        "outputId": "c4311eab-1a1b-430a-9918-f423877b4483",
        "colab": {
          "referenced_widgets": [
            "bb7c34fc5f384645b9a37f5ab99bbf13"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size : 256000\n",
            "PAD TOKEN : <pad>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb7c34fc5f384645b9a37f5ab99bbf13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type = 'nf4',\n",
        "    bnb_4bit_use_double_quant = True\n",
        ")\n",
        "\n",
        "\n",
        "model_name = \"google/gemma-2b\"\n",
        "\n",
        "\n",
        "## tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Vocab size : {tokenizer.vocab_size}\")\n",
        "print(f\"PAD TOKEN : {tokenizer.pad_token}\")\n",
        "\n",
        "\n",
        "## model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config,\n",
        "                             device_map = {\"\":0}, token = os.environ['HF_READ_TOKEN'])\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OJHjQ30_erL"
      },
      "source": [
        "## Inference Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvcWOWvD_erL",
        "outputId": "14c07783-ab50-49e6-e6de-a0ffedc3704d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
            "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\n",
            "INSERT INTO farm VALUES (10, 100);\n",
            "INSERT INTO farm VALUES (\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
        "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\"\"\"\n",
        "\n",
        "answer = \"SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\"\n",
        "dash_line = dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens = 20)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIeFvihg_erM"
      },
      "source": [
        "## Traning Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqOKHHBm_erM"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZuyZm06_erN",
        "outputId": "76a32558-b4a6-475b-ad08-a8eeecbec334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GemmaForCausalLM(\n",
            "  (model): GemmaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x GemmaDecoderLayer(\n",
            "        (self_attn): GemmaAttention(\n",
            "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): GemmaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): GemmaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
            "          (act_fn): GELUActivation()\n",
            "        )\n",
            "        (input_layernorm): GemmaRMSNorm()\n",
            "        (post_attention_layernorm): GemmaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): GemmaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pqmzldI_erN",
        "outputId": "a6e72596-2272-427e-8aa4-6ea9df144db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 39223296 || all params: 1554491392 || trainable%: 2.5232237503441897\n"
          ]
        }
      ],
      "source": [
        "## Lora config\n",
        "lora_config = LoraConfig(\n",
        "    r= 32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwVY3xZk_erO"
      },
      "source": [
        "## Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH8Ac4kh_erO",
        "outputId": "47a5cb4c-bfec-4f82-e9b4-132c3e5cdccb",
        "colab": {
          "referenced_widgets": [
            "0d0d79ea919e483398631e7ccf5197e1",
            "0c5ca5369ad9496886c59515e0bc2755"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d0d79ea919e483398631e7ccf5197e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/70719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c5ca5369ad9496886c59515e0bc2755",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7858 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "CHECKPOINTS_DIR = \"checkpoints\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=CHECKPOINTS_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    max_steps=200,\n",
        "\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.05,\n",
        "    group_by_length=True,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_safetensors=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. re-enable for inference!\n",
        "\n",
        "## SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=transformed_dataset['train'],\n",
        "    eval_dataset= transformed_dataset['test'],\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqkmLBDf_erP"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u09P-Z27_erQ",
        "outputId": "bcd4c8f6-e587-4044-a703-36d9165571fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 1:54:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.207400</td>\n",
              "      <td>1.101300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.122000</td>\n",
              "      <td>1.045500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.036000</td>\n",
              "      <td>1.018756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.931100</td>\n",
              "      <td>1.020099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.850900</td>\n",
              "      <td>1.057535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.096900</td>\n",
              "      <td>1.004143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.080500</td>\n",
              "      <td>1.000473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.942500</td>\n",
              "      <td>0.976737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.878500</td>\n",
              "      <td>0.973977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>0.999553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.094500</td>\n",
              "      <td>0.964102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.007800</td>\n",
              "      <td>0.948006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.958800</td>\n",
              "      <td>0.944077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.853200</td>\n",
              "      <td>0.943771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.821900</td>\n",
              "      <td>0.961391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.087500</td>\n",
              "      <td>0.936069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.983700</td>\n",
              "      <td>0.935450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.904800</td>\n",
              "      <td>0.931899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.876500</td>\n",
              "      <td>0.929377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.838800</td>\n",
              "      <td>0.929039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.9700161218643188, metrics={'train_runtime': 6855.1614, 'train_samples_per_second': 0.467, 'train_steps_per_second': 0.029, 'total_flos': 2784617530146816.0, 'train_loss': 0.9700161218643188, 'epoch': 0.05})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## ran for 200 steps\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOEfQJG0_erQ"
      },
      "source": [
        "# Save trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALQKE7R_erQ"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9gKPA_k_erR"
      },
      "outputs": [],
      "source": [
        "## If you will get the access error then uncomment and run above cell\n",
        "## This will only save the adapter\n",
        "peft_model_path = \"finetuned-adapters\"\n",
        "tokenizer.save_pretrained(peft_model_path)\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUM1jcYd_erR"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmx0fAvo_erR"
      },
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s93cC2cV_erS",
        "outputId": "1234faf5-8ad8-4580-e1d8-fd4b98dcb28e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GemmaForCausalLM(\n",
              "      (model): GemmaModel(\n",
              "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x GemmaDecoderLayer(\n",
              "            (self_attn): GemmaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): GemmaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): GemmaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=16384, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): GELUActivation()\n",
              "            )\n",
              "            (input_layernorm): GemmaRMSNorm()\n",
              "            (post_attention_layernorm): GemmaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): GemmaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYyRAvp4_erS",
        "outputId": "b20a23b2-11de-4041-efc2-582b9121a6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
            "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\n",
            "Answer: SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
        "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\"\"\"\n",
        "\n",
        "answer = \"SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\"\n",
        "dash_line = dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens = 20)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16mUPSUb_erT",
        "outputId": "9be4644e-d84a-4e5a-9279-a9dd0713c793"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del model\n",
        "del tokenizer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fj3M7uD_erU"
      },
      "source": [
        "## Save Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-fGJ9VV_erU",
        "outputId": "70a93f17-972d-42ce-db6a-b6d852f7777a",
        "colab": {
          "referenced_widgets": [
            "7f1ac098710e4ba3874e114352e520df"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f1ac098710e4ba3874e114352e520df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\":0},\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ_vbZyB_erU"
      },
      "outputs": [],
      "source": [
        "## Save Full Model\n",
        "complete_model_path = \"finetuned-models\"\n",
        "trainer.model.save_pretrained(complete_model_path)\n",
        "model.save_pretrained(complete_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt9A2fk3_erV",
        "outputId": "fab58e0c-9f51-43f4-8a1e-a356e471e234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
            "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\n",
            "Answer: SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
        "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\"\"\"\n",
        "\n",
        "answer = \"SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\"\n",
        "dash_line = dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens = 20)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaNkaPfU_erW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lighting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}