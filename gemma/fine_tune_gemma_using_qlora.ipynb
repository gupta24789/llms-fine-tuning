{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta24789/llms-fine-tuning/blob/main/gemma/fine_tune_gemma_using_qlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjKxHTfY9n65"
      },
      "source": [
        "## Objective\n",
        "\n",
        "- Fine tune **google/gemma-2b** model.\n",
        "- Dataset : **b-mc2/sql-create-context**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewFB3Gnu9n69"
      },
      "outputs": [],
      "source": [
        "# !pip3 install -q -U bitsandbytes==0.42.0\n",
        "# !pip3 install -q -U peft==0.8.2\n",
        "# !pip3 install -q -U trl==0.7.10\n",
        "# !pip3 install -q -U accelerate==0.27.1\n",
        "# !pip3 install -q -U datasets==2.17.0\n",
        "# !pip3 install -q -U transformers==4.38.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHe_5CZ9n6_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKv9dNTK9n7A",
        "outputId": "8c18d24a-6209-40bc-889b-cfea63831842"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from pprint import pprint\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments,pipeline,logging\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftModel\n",
        "from trl import SFTTrainer\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HjYptA29n7B"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVz4qDSH9n7C",
        "outputId": "ff3fd4f8-0062-4ec3-fda1-b56318453551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['context', 'answer', 'question'],\n",
              "    num_rows: 78577\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbDB1O_I9n7D",
        "outputId": "f5fc27ab-6992-4cbe-c199-5ce38b3dd18c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 70719\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 7858\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Create Train & Test Split\n",
        "dataset = dataset.train_test_split(test_size = 0.1, seed=42)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LWRzJx99n7E",
        "outputId": "6dbb752f-9a72-41ee-f0f1-51e8a70d2edc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE table_name_75 (insurgents VARCHAR, civilians VARCHAR)',\n",
              " 'answer': 'SELECT insurgents FROM table_name_75 WHERE civilians = \"49\"',\n",
              " 'question': 'Name the insurgents for civilians being 49'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx4YM_qC9n7F"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "Data Format:\n",
        "\n",
        "        Question : <question>\n",
        "        Context : <context>\n",
        "        Answer : <answer>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBjaxHPf9n7F"
      },
      "outputs": [],
      "source": [
        "def transform_data(row):\n",
        "    text = f\"Question : {row['question']}\\nContext : {row['context']}\\nAnswer : {row['answer']}\"\n",
        "    return {\"text\":text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQkf_YYF9n7G",
        "outputId": "260ed422-9f1a-4517-b71b-e9a536e0b686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question', 'text'],\n",
              "        num_rows: 70719\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['context', 'answer', 'question', 'text'],\n",
              "        num_rows: 7858\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformed_dataset = dataset.map(transform_data)\n",
        "transformed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHJoqb9R9n7H",
        "outputId": "734b12ba-8977-4656-8b7a-990a83daa5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question : Name the insurgents for civilians being 49\n",
            "Context : CREATE TABLE table_name_75 (insurgents VARCHAR, civilians VARCHAR)\n",
            "Answer : SELECT insurgents FROM table_name_75 WHERE civilians = \"49\"\n"
          ]
        }
      ],
      "source": [
        "print(transformed_dataset['train'][0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuJCjyJd9n7I"
      },
      "source": [
        "## Load Model & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNSpposY9n7I",
        "outputId": "b31fc503-bf0f-4386-d82e-01e122b2e103",
        "colab": {
          "referenced_widgets": [
            "0f88bd47a8464f23912b7da3293cd72c",
            "db1524157f87400fbbadd85c6ed9c384"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size : 256000\n",
            "PAD TOKEN : <pad>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f88bd47a8464f23912b7da3293cd72c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db1524157f87400fbbadd85c6ed9c384",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type = 'nf4',\n",
        "    bnb_4bit_use_double_quant = True\n",
        ")\n",
        "\n",
        "\n",
        "model_name = \"google/gemma-2b\"\n",
        "\n",
        "\n",
        "## tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Vocab size : {tokenizer.vocab_size}\")\n",
        "print(f\"PAD TOKEN : {tokenizer.pad_token}\")\n",
        "\n",
        "\n",
        "## model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config,\n",
        "                             device_map = {\"\":0}, token = os.environ['HF_READ_TOKEN'])\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMfGSij59n7J"
      },
      "source": [
        "## Inference Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RkaknlS9n7J",
        "outputId": "368fb8c2-117a-4c46-ff7b-23aa097f402c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : \n",
            "Question: Find the name and training hours of players whose hours are below 1500?\n",
            "Context: CREATE TABLE Player (pName VARCHAR, HS INTEGER)\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Answer : SELECT pName, HS FROM Player WHERE HS < 1500\n",
            "---------------------------------------------------------------------------------------------------\n",
            " SELECT rows. It also specifies a relationship between the rows of the table from which you want to SELECT rows and the rows of another table or view that you specify in a FROM clause.\n",
            "WHERE : The WHERE clause is used to filter the rows from the FROM clause in a SELECT statement.\n",
            "HAVING : The HAVING clause is used to add a condition to a group. It is not used in the SELECT clause, and it is not used in a UNION statement.\n",
            "ORDER BY\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Question: Find the name and training hours of players whose hours are below 1500?\n",
        "Context: CREATE TABLE Player (pName VARCHAR, HS INTEGER)\"\"\"\n",
        "answer = \"SELECT pName, HS FROM Player WHERE HS < 1500\"\n",
        "dash_line = dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200, do_sample = True, top_k = 10)\n",
        "\n",
        "\n",
        "print(f\"Input : \\n{text}\")\n",
        "print(dash_line)\n",
        "print(f\"Answer : {answer}\")\n",
        "print(dash_line)\n",
        "print(tokenizer.decode(outputs[0][len(text):], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INRCrxLs9n7J"
      },
      "source": [
        "## Traning Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU-8dwQu9n7K"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQYnpdBE9n7K",
        "outputId": "cbb76ac6-cecf-4f7a-ed29-ab179465253b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GemmaForCausalLM(\n",
            "  (model): GemmaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x GemmaDecoderLayer(\n",
            "        (self_attn): GemmaAttention(\n",
            "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): GemmaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): GemmaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
            "          (act_fn): GELUActivation()\n",
            "        )\n",
            "        (input_layernorm): GemmaRMSNorm()\n",
            "        (post_attention_layernorm): GemmaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): GemmaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkFwIGdP9n7L",
        "outputId": "4db50198-0661-4163-db60-24908e1b68d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 39223296 || all params: 1554491392 || trainable%: 2.5232237503441897\n"
          ]
        }
      ],
      "source": [
        "## Lora config\n",
        "lora_config = LoraConfig(\n",
        "    r= 32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjXEqEu09n7L"
      },
      "source": [
        "## Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVukcVLT9n7L",
        "outputId": "d9a02181-8466-4b20-9221-c6aac367c7e9",
        "colab": {
          "referenced_widgets": [
            "0d0d79ea919e483398631e7ccf5197e1",
            "0c5ca5369ad9496886c59515e0bc2755"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d0d79ea919e483398631e7ccf5197e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/70719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c5ca5369ad9496886c59515e0bc2755",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7858 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "CHECKPOINTS_DIR = \"checkpoints\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=CHECKPOINTS_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    max_steps=200,\n",
        "\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.05,\n",
        "    group_by_length=True,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_safetensors=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. re-enable for inference!\n",
        "\n",
        "## SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=transformed_dataset['train'],\n",
        "    eval_dataset= transformed_dataset['test'],\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfMGKGiP9n7M"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPC7zp8L9n7M",
        "outputId": "dc71047f-27e7-42c8-fcff-fc9013feeeb4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 1:54:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.207400</td>\n",
              "      <td>1.101300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.122000</td>\n",
              "      <td>1.045500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.036000</td>\n",
              "      <td>1.018756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.931100</td>\n",
              "      <td>1.020099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.850900</td>\n",
              "      <td>1.057535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.096900</td>\n",
              "      <td>1.004143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.080500</td>\n",
              "      <td>1.000473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.942500</td>\n",
              "      <td>0.976737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.878500</td>\n",
              "      <td>0.973977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>0.999553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.094500</td>\n",
              "      <td>0.964102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.007800</td>\n",
              "      <td>0.948006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.958800</td>\n",
              "      <td>0.944077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.853200</td>\n",
              "      <td>0.943771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.821900</td>\n",
              "      <td>0.961391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.087500</td>\n",
              "      <td>0.936069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.983700</td>\n",
              "      <td>0.935450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.904800</td>\n",
              "      <td>0.931899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.876500</td>\n",
              "      <td>0.929377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.838800</td>\n",
              "      <td>0.929039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.9700161218643188, metrics={'train_runtime': 6855.1614, 'train_samples_per_second': 0.467, 'train_steps_per_second': 0.029, 'total_flos': 2784617530146816.0, 'train_loss': 0.9700161218643188, 'epoch': 0.05})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## ran for 200 steps\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JshGigBp9n7M"
      },
      "source": [
        "# Save trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiVdWg6U9n7M"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8boJQUFp9n7N"
      },
      "outputs": [],
      "source": [
        "## If you will get the access error then uncomment and run above cell\n",
        "## This will only save the adapter\n",
        "peft_model_path = \"finetuned-adapters\"\n",
        "tokenizer.save_pretrained(peft_model_path)\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEIDzX79n7N"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl1tQQS79n7N"
      },
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfkGuYq29n7N",
        "outputId": "8b3b5dd4-c604-4b2c-ec6b-a5a8462d1396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GemmaForCausalLM(\n",
              "      (model): GemmaModel(\n",
              "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x GemmaDecoderLayer(\n",
              "            (self_attn): GemmaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): GemmaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): GemmaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=16384, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): GELUActivation()\n",
              "            )\n",
              "            (input_layernorm): GemmaRMSNorm()\n",
              "            (post_attention_layernorm): GemmaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): GemmaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnkcvdNO9n7N",
        "outputId": "0bd58117-f54b-4ed6-8f75-ead6c82d58d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
            "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\n",
            "Answer: SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
        "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\"\"\"\n",
        "\n",
        "answer = \"SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\"\n",
        "dash_line = dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens = 20)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP16SlFh9n7O",
        "outputId": "cae13e25-6133-4075-8910-9a776ec4de8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del model\n",
        "del tokenizer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB2AO3sS9n7O"
      },
      "source": [
        "## Save Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQBrsiNG9n7O",
        "outputId": "5182b544-d8d2-45f4-a6ec-d67d3ccf2759",
        "colab": {
          "referenced_widgets": [
            "7f1ac098710e4ba3874e114352e520df"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f1ac098710e4ba3874e114352e520df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\":0},\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7t0kgI59n7P"
      },
      "outputs": [],
      "source": [
        "## Save Full Model\n",
        "complete_model_path = \"finetuned-models\"\n",
        "trainer.model.save_pretrained(complete_model_path)\n",
        "model.save_pretrained(complete_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQgbUVd89n7P",
        "outputId": "9d6b0c4e-18b4-48b4-f0bc-a3a3dd051350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
            "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\n",
            "Answer: SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Question: What is the average number of working horses of farms with greater than 45 total number of horses?\n",
        "Context: CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)\"\"\"\n",
        "\n",
        "answer = \"SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses > 45\"\n",
        "dash_line = dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens = 20)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5Ge74ZR9n7R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lighting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}